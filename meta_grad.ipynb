{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Meta-gradient learning with meta optimizer\n",
    "\n",
    "<img src=\"img/alg.png\" alt=\"algorithm\" style=\"width:800px;\"/>\n",
    "\n",
    "Optimize gamma with an Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import higher\n",
    "import torch\n",
    "from torch.distributions import Categorical\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from models.models import ActorCritic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ITERATION_NUMS = 1000  # 500\n",
    "SAMPLE_NUMS = 50  # 100\n",
    "LR = 0.01\n",
    "LAMBDA = torch.tensor(0.98)\n",
    "CLIP_GRAD_NORM = 40  # 40\n",
    "MU = 0.\n",
    "BETA = 0.001\n",
    "GAMMA_INIT = 0.99  # 0.99\n",
    "GAMMA_FIX = 1.  # 0.995"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Function\n",
    "### Rollout function & testing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roll_out(agent, task, sample_nums, init_state):\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    vts = []  # v-values at timestep t\n",
    "    vt1s = []  # v-values at timestep t+1\n",
    "    dones = []\n",
    "\n",
    "    state = init_state\n",
    "\n",
    "    for i in range(sample_nums):\n",
    "        states.append(state)\n",
    "        act, vt = choose_action(agent, state)\n",
    "        actions.append(act)\n",
    "\n",
    "        next_state, reward, done, _ = task.step(act.numpy())\n",
    "        with torch.no_grad():\n",
    "            _, vt1 = agent(torch.Tensor(next_state))\n",
    "        state = next_state\n",
    "        rewards.append(reward)\n",
    "        dones.append(1 if done is False else 0)\n",
    "        vt = vt.detach().numpy()\n",
    "        vt1 = vt1.detach().numpy()\n",
    "        vts.append(vt)\n",
    "        vt1s.append(vt1)\n",
    "\n",
    "        if done:\n",
    "            state = task.reset()\n",
    "\n",
    "    return states, actions, rewards, vts, vt1s, dones, state\n",
    "\n",
    "\n",
    "def test(gym_name, agent):\n",
    "    result = 0\n",
    "    test_task = gym.make(gym_name)\n",
    "    for test_epi in range(10):\n",
    "        state = test_task.reset()\n",
    "        for test_step in range(500):\n",
    "            act, _ = choose_action(agent, state)\n",
    "            next_state, reward, done, _ = test_task.step(act.numpy())\n",
    "            result += reward\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computational function\n",
    "Functions to compute advantages and target v-values from a trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def choose_action(agent, state):\n",
    "    logits, v = agent(torch.Tensor(state))\n",
    "    act_probs = F.softmax(logits, dim=-1)\n",
    "    m = Categorical(act_probs)\n",
    "    act = m.sample()\n",
    "\n",
    "    return act, v\n",
    "\n",
    "\n",
    "def gae_calculater_grad(rewards, v_t_s, v_t1_s, dones, gamma, lambda_):\n",
    "    \"\"\"\n",
    "    Calculate advantages and target v-values\n",
    "    \"\"\"\n",
    "    batch_size = len(rewards)\n",
    "    R = torch.reshape(torch.tensor(0), (1, 1))  # increment term\n",
    "    advs = torch.zeros(batch_size)\n",
    "    for t in reversed(range(0, batch_size)):\n",
    "        delta = torch.tensor(rewards[t]) - torch.tensor(v_t_s[t]) + \\\n",
    "                (gamma * torch.tensor(v_t1_s[t]) * torch.tensor(dones[t]))\n",
    "        R = delta + (gamma * lambda_ * R * torch.tensor(dones[t]))\n",
    "        advs[t] = R\n",
    "    value_target = advs + torch.tensor(np.squeeze(v_t_s))  # target v is calculated from adv.\n",
    "\n",
    "    return advs, value_target\n",
    "\n",
    "\n",
    "def trajectory_cutter(rewards, vts, vt1s, dones):\n",
    "    \"\"\"\n",
    "    Divide sample into multiple groups for returns computing.\n",
    "    Samples in the same group means that they was sampled in the same epoch.\n",
    "    \"\"\"\n",
    "    # \"not done\" = 1, \"done\" = 0\n",
    "    cutted_rewards = []\n",
    "    cutted_vts = []\n",
    "    cutted_vt1s = []\n",
    "    cutted_dones = []\n",
    "    temp_r = []\n",
    "    temp_vt = []\n",
    "    temp_vt1 = []\n",
    "    temp_d = []\n",
    "    \n",
    "    for (reward, vt, vt1, done) in zip(rewards, vts, vt1s, dones):\n",
    "        temp_r.append(reward)\n",
    "        temp_vt.append(vt)\n",
    "        temp_vt1.append(vt1)\n",
    "        temp_d.append(done)\n",
    "        if done == 0:\n",
    "            cutted_rewards.append(temp_r)\n",
    "            cutted_vts.append(temp_vt)\n",
    "            cutted_vt1s.append(temp_vt1)\n",
    "            cutted_dones.append(temp_d)\n",
    "            temp_r = []\n",
    "            temp_vt = []\n",
    "            temp_vt1 = []\n",
    "            temp_d = []\n",
    "    cutted_rewards.append(temp_r)\n",
    "    cutted_vts.append(temp_vt)\n",
    "    cutted_vt1s.append(temp_vt1)\n",
    "    cutted_dones.append(temp_d)\n",
    "    \n",
    "    return cutted_rewards, cutted_vts, cutted_vt1s, cutted_dones\n",
    "\n",
    "\n",
    "def trajectory_gae(rewards, vts, vt1s, dones, gamma, lambda_):\n",
    "    rewards, vts, vt1s, dones = trajectory_cutter(rewards, vts, vt1s, dones)\n",
    "    \n",
    "    advs = []\n",
    "    v_targets = []\n",
    "    for (r, vt, vt1, d) in zip(rewards, vts, vt1s, dones):\n",
    "        adv, v_target = gae_calculater_grad(r, vt, vt1, d, gamma, lambda_)\n",
    "        advs.append(adv)\n",
    "        v_targets.append(v_target)\n",
    "    advs = torch.cat(advs).float()\n",
    "    v_targets = torch.cat(v_targets).float()\n",
    "    \n",
    "    return advs, v_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test gae_calculater_grad()\n",
    "rewards = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
    "dones = [1, 1, 1, 1, 1, 0, 1, 1, 1, 1]\n",
    "vts = [np.array(np.random.randn()) for _ in range(10)]\n",
    "vt1s = [np.array(np.random.randn()) for _ in range(10)]\n",
    "GAMMA = torch.tensor(0.99, requires_grad=True)\n",
    "LAMBDA = torch.tensor(0.98)\n",
    "\n",
    "advs, v_targets = trajectory_gae(rewards, vts, vt1s, dones, GAMMA, LAMBDA)\n",
    "# grad = torch.autograd.grad(advs.mean(), GAMMA, allow_unused=True)\n",
    "\n",
    "# for g in grad:\n",
    "#     assert g is not None, \"test failed\"\n",
    "#     print(\"test passed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent training function\n",
    "RL algorithm: A2C + GAE\n",
    "Calculate grad for substitute agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logits(agent, states, actions, action_dim):\n",
    "    states = torch.Tensor(np.array(states))\n",
    "    actions = torch.tensor(actions, dtype=torch.int64).view(-1, 1)\n",
    "    \n",
    "    logits, v = agent(states)\n",
    "    logits = logits.view(-1, action_dim)\n",
    "    v = v.view(-1)\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    log_probs = F.log_softmax(logits, dim=1)\n",
    "    log_probs_act = log_probs.gather(1, actions).view(-1)\n",
    "    \n",
    "    return probs, log_probs, log_probs_act, v\n",
    "\n",
    "\n",
    "def get_meta_logits(agent, states, actions, action_dim):\n",
    "    states = torch.Tensor(np.array(states))\n",
    "    actions = torch.tensor(actions, dtype=torch.int64).view(-1, 1)\n",
    "    \n",
    "    logits, _ = agent(states)\n",
    "    logits = logits.view(-1, action_dim)\n",
    "    log_probs = F.log_softmax(logits, dim=1)\n",
    "    log_probs_act = log_probs.gather(1, actions).view(-1)\n",
    "    \n",
    "    return log_probs_act"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[123456789]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_seed = 123456789\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "gym_name = \"CartPole-v1\"\n",
    "task = gym.make(gym_name)\n",
    "task.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "discrete = isinstance(task.action_space, gym.spaces.Discrete)\n",
    "STATE_DIM = task.observation_space.shape[0]\n",
    "ACTION_DIM = task.action_space.n if discrete else task.action_space.shape[0]\n",
    "\n",
    "agent = ActorCritic(STATE_DIM, ACTION_DIM)\n",
    "optim = torch.optim.Adam(agent.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.LinearLR(optim, start_factor=1.0, end_factor=0.0, total_iters=ITERATION_NUMS)\n",
    "assert next(agent.parameters()).is_cuda is False  # use only cpu for the current version\n",
    "\n",
    "gamma = torch.tensor(GAMMA_INIT, requires_grad=True)\n",
    "meta_optim = torch.optim.Adam([gamma], lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test *higher.innerloop_ctx()* : \n",
    "\n",
    "Q: Will the parameters of the original agent be updated when the differentiable optimizer update the monkey-patched agent?\n",
    "\n",
    "A: No."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 100 test result: 13.8 gamma: tensor(0.9900)\n",
      "iteration: 200 test result: 11.9 gamma: tensor(0.9900)\n",
      "iteration: 300 test result: 15.4 gamma: tensor(0.9900)\n",
      "iteration: 400 test result: 11.7 gamma: tensor(0.9900)\n",
      "iteration: 500 test result: 13.9 gamma: tensor(0.9900)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m diffopt \u001b[38;5;241m=\u001b[39m higher\u001b[38;5;241m.\u001b[39mget_diff_optim(optim, agent\u001b[38;5;241m.\u001b[39mparameters(), fmodel\u001b[38;5;241m=\u001b[39mfmodel)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(ITERATION_NUMS):\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# Line 3-4　Sample trajectories for both RL and meta-grad learning\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m     states1, actions1, rewards1, vts1, vt1s1, dones1, current_state \u001b[38;5;241m=\u001b[39m \u001b[43mroll_out\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSAMPLE_NUMS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     init_state \u001b[38;5;241m=\u001b[39m current_state\n\u001b[0;32m     12\u001b[0m     states2, actions2, rewards2, vts2, vt1s2, dones2, current_state \u001b[38;5;241m=\u001b[39m roll_out(fmodel, task, SAMPLE_NUMS, init_state)\n",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36mroll_out\u001b[1;34m(agent, task, sample_nums, init_state)\u001b[0m\n\u001b[0;32m     16\u001b[0m next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m task\u001b[38;5;241m.\u001b[39mstep(act\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 18\u001b[0m     _, vt1 \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[0;32m     20\u001b[0m rewards\u001b[38;5;241m.\u001b[39mappend(reward)\n",
      "File \u001b[1;32mD:\\Soft\\Anaconda\\envs\\curiosityrl\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\Soft\\Anaconda\\envs\\curiosityrl\\lib\\site-packages\\higher\\patch.py:460\u001b[0m, in \u001b[0;36mmake_functional.<locals>._patched_forward\u001b[1;34m(self, params, *args, **kwargs)\u001b[0m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_patched_forward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_refill_params_box(params)\n\u001b[1;32m--> 460\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mboxed_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    462\u001b[0m     \u001b[38;5;66;03m# Clean up\u001b[39;00m\n\u001b[0;32m    463\u001b[0m     params_box[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Soft\\Anaconda\\envs\\curiosityrl\\lib\\site-packages\\higher\\patch.py:387\u001b[0m, in \u001b[0;36m_make_functional.<locals>.patched_forward\u001b[1;34m(self, params, *args, **kwargs)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_RNN \u001b[38;5;129;01mand\u001b[39;00m _torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m    385\u001b[0m     _warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, category\u001b[38;5;241m=\u001b[39m\u001b[38;5;167;01mUserWarning\u001b[39;00m)\n\u001b[1;32m--> 387\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrue_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Research\\Code\\Meta_grad_RL\\models\\models.py:32\u001b[0m, in \u001b[0;36mActorCritic.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[0;32m     31\u001b[0m     pi \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor(inputs)\n\u001b[1;32m---> 32\u001b[0m     vi \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcritic\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pi, vi\n",
      "File \u001b[1;32mD:\\Soft\\Anaconda\\envs\\curiosityrl\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\Soft\\Anaconda\\envs\\curiosityrl\\lib\\site-packages\\higher\\patch.py:387\u001b[0m, in \u001b[0;36m_make_functional.<locals>.patched_forward\u001b[1;34m(self, params, *args, **kwargs)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_RNN \u001b[38;5;129;01mand\u001b[39;00m _torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m    385\u001b[0m     _warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, category\u001b[38;5;241m=\u001b[39m\u001b[38;5;167;01mUserWarning\u001b[39;00m)\n\u001b[1;32m--> 387\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrue_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Soft\\Anaconda\\envs\\curiosityrl\\lib\\site-packages\\torch\\nn\\modules\\container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 141\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mD:\\Soft\\Anaconda\\envs\\curiosityrl\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\Soft\\Anaconda\\envs\\curiosityrl\\lib\\site-packages\\higher\\patch.py:387\u001b[0m, in \u001b[0;36m_make_functional.<locals>.patched_forward\u001b[1;34m(self, params, *args, **kwargs)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_RNN \u001b[38;5;129;01mand\u001b[39;00m _torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m    385\u001b[0m     _warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, category\u001b[38;5;241m=\u001b[39m\u001b[38;5;167;01mUserWarning\u001b[39;00m)\n\u001b[1;32m--> 387\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrue_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Soft\\Anaconda\\envs\\curiosityrl\\lib\\site-packages\\torch\\nn\\modules\\linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Soft\\Anaconda\\envs\\curiosityrl\\lib\\site-packages\\torch\\nn\\functional.py:1848\u001b[0m, in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[0;32m   1847\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(linear, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, weight, bias\u001b[38;5;241m=\u001b[39mbias)\n\u001b[1;32m-> 1848\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "iterations = []\n",
    "test_results = []\n",
    "gamma_buffer = []\n",
    "init_state = task.reset()\n",
    "\n",
    "fmodel = higher.monkeypatch(agent)\n",
    "diffopt = higher.get_diff_optim(optim, agent.parameters(), fmodel=fmodel)\n",
    "for i in range(ITERATION_NUMS):\n",
    "    # Line 3-4　Sample trajectories for both RL and meta-grad learning\n",
    "    states1, actions1, rewards1, vts1, vt1s1, dones1, current_state = roll_out(fmodel, task, SAMPLE_NUMS, init_state)\n",
    "    init_state = current_state\n",
    "    states2, actions2, rewards2, vts2, vt1s2, dones2, current_state = roll_out(fmodel, task, SAMPLE_NUMS, init_state)\n",
    "    init_state = current_state\n",
    "\n",
    "    \"\"\"RL Phase:\"\"\"\n",
    "    probs, log_probs, log_probs_act, v = get_logits(fmodel, states1, actions1, ACTION_DIM)\n",
    "    advs, v_targets = trajectory_gae(rewards1, vts1, vt1s1, dones1, gamma, LAMBDA)\n",
    "\n",
    "    # Line 5  Loss computation\n",
    "    loss_policy = - (advs * log_probs_act).mean()\n",
    "    loss_critic = F.mse_loss(v_targets, v, reduction='mean')\n",
    "    loss_entropy = - (log_probs * probs).mean()\n",
    "\n",
    "    loss = loss_policy + .25 * loss_critic - .001 * loss_entropy\n",
    "    torch.nn.utils.clip_grad_norm_(fmodel.parameters(), CLIP_GRAD_NORM)\n",
    "\n",
    "    # Line 6  Obtain the update agent\n",
    "    diffopt.step(loss)\n",
    "    \n",
    "    agent.load_state_dict(fmodel.state_dict())\n",
    "    fmodel = higher.monkeypatch(agent)\n",
    "\n",
    "    # testing\n",
    "    if (i + 1) % (ITERATION_NUMS // 10) == 0:\n",
    "        result = test(gym_name, fmodel)\n",
    "        print(\"iteration:\", i + 1, \"test result:\", result / 10.0, \"gamma:\", gamma.data)\n",
    "        iterations.append(i + 1)\n",
    "        test_results.append(result / 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = []\n",
    "test_results = []\n",
    "gamma_buffer = []\n",
    "init_state = task.reset()\n",
    "for i in range(ITERATION_NUMS):\n",
    "    with higher.innerloop_ctx(agent, optim) as (fmodel, diffopt):\n",
    "        # Line 3-4　Sample trajectories for both RL and meta-grad learning\n",
    "        states1, actions1, rewards1, vts1, vt1s1, dones1, current_state = roll_out(fmodel, task, SAMPLE_NUMS, init_state)\n",
    "        init_state = current_state\n",
    "        states2, actions2, rewards2, vts2, vt1s2, dones2, current_state = roll_out(fmodel, task, SAMPLE_NUMS, init_state)\n",
    "        init_state = current_state\n",
    "\n",
    "        \"\"\"RL Phase:\"\"\"\n",
    "        probs, log_probs, log_probs_act, v = get_logits(fmodel, states1, actions1, ACTION_DIM)\n",
    "        advs, v_targets = trajectory_gae(rewards1, vts1, vt1s1, dones1, gamma, LAMBDA)\n",
    "\n",
    "        # Line 5  Loss computation\n",
    "        loss_policy = - (advs * log_probs_act).mean()\n",
    "        loss_critic = F.mse_loss(v_targets, v, reduction='mean')\n",
    "        loss_entropy = - (log_probs * probs).mean()\n",
    "\n",
    "        loss = loss_policy + .25 * loss_critic - .001 * loss_entropy\n",
    "        ## no need to call zero_grad() and loss.backwards()\n",
    "        # diffopt.zero_grad()  \n",
    "        # loss.backward(retain_graph=True)\n",
    "        torch.nn.utils.clip_grad_norm_(fmodel.parameters(), CLIP_GRAD_NORM)\n",
    "\n",
    "        # Line 6  Obtain the update agent\n",
    "        diffopt.step(loss)\n",
    "\n",
    "        \"\"\"Meta-grad Learning Phase:\"\"\"\n",
    "        # Line 7-9  Update meta-parameter\n",
    "        log_probs_act_dash = get_meta_logits(fmodel, states2, actions2, ACTION_DIM)\n",
    "        advs_dash, _ = trajectory_gae(rewards2, vts2, vt1s2, dones2, gamma, LAMBDA)\n",
    "\n",
    "        meta_loss = (advs_dash.detach() * log_probs_act_dash).mean()\n",
    "\n",
    "        meta_optim.zero_grad()\n",
    "        meta_loss.backward()\n",
    "        meta_optim.step()\n",
    "\n",
    "        # testing\n",
    "        if (i + 1) % (ITERATION_NUMS // 100) == 0:\n",
    "            result = test(gym_name, fmodel)\n",
    "            print(\"iteration:\", i + 1, \"test result:\", result / 10.0, \"gamma:\", gamma.data)\n",
    "            iterations.append(i + 1)\n",
    "            test_results.append(result / 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original version without *higher*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = []\n",
    "test_results = []\n",
    "gamma_buffer = []\n",
    "init_state = task.reset()\n",
    "for i in range(ITERATION_NUMS):\n",
    "    # Line 3-4　Sample trajectories for both RL and meta-grad learning\n",
    "    states1, actions1, rewards1, vts1, vt1s1, dones1, current_state = roll_out(agent, task, SAMPLE_NUMS, init_state)\n",
    "    init_state = current_state\n",
    "    states2, actions2, rewards2, vts2, vt1s2, dones2, current_state = roll_out(agent, task, SAMPLE_NUMS, init_state)\n",
    "    init_state = current_state\n",
    "\n",
    "    \"\"\"RL Phase:\"\"\"\n",
    "    probs, log_probs, log_probs_act, v = get_logits(agent, states1, actions1, ACTION_DIM)\n",
    "    advs, v_targets = trajectory_gae(rewards1, vts1, vt1s1, dones1, gamma, LAMBDA)\n",
    "\n",
    "    # Line 5  Loss computation\n",
    "    loss_policy = - (advs * log_probs_act).mean()\n",
    "    loss_critic = F.mse_loss(v_targets, v, reduction='mean')\n",
    "    loss_entropy = - (log_probs * probs).mean()\n",
    "\n",
    "    loss = loss_policy + .25 * loss_critic - .001 * loss_entropy\n",
    "    optim.zero_grad()\n",
    "    loss.backward(retain_graph=True)\n",
    "    torch.nn.utils.clip_grad_norm_(agent.parameters(), CLIP_GRAD_NORM)\n",
    "\n",
    "    # Line 6  Obtain the update agent\n",
    "    optim.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    \"\"\"Meta-grad Learning Phase:\"\"\"\n",
    "    # Line 7-9  Update meta-parameter\n",
    "    log_probs_act_dash = get_meta_logits(agent, states2, actions2, ACTION_DIM)\n",
    "    advs_dash, _ = trajectory_gae(rewards2, vts2, vt1s2, dones2, gamma, LAMBDA)\n",
    "\n",
    "    meta_loss = (advs_dash.detach() * log_probs_act_dash).mean()\n",
    "#     grad = torch.autograd.grad(log_probs_act_dash.mean(), gamma, retain_graph=True, allow_unused=True)\n",
    "#     print(grad)\n",
    "#     for g in grad:\n",
    "#         assert g is not None, \"test failed\"\n",
    "#         print(\"test passed\")\n",
    "\n",
    "    meta_optim.zero_grad()\n",
    "    meta_loss.backward()\n",
    "    meta_optim.step()\n",
    "\n",
    "    # testing\n",
    "    if (i + 1) % (ITERATION_NUMS // 10) == 0:\n",
    "        result = test(gym_name, agent)\n",
    "        print(\"iteration:\", i + 1, \"test result:\", result / 10.0, \"gamma:\", gamma.data)\n",
    "        iterations.append(i + 1)\n",
    "        test_results.append(result / 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
