{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Meta-gradient learning with SGD\n",
    "\n",
    "<img src=\"img/alg.png\" alt=\"algorithm\" style=\"width:800px;\"/>\n",
    "\n",
    "Optimize gamma with an Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import higher\n",
    "import torch\n",
    "from torch.distributions import Categorical\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from models.models import ActorCritic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ITERATION_NUMS = 20000  # 500\n",
    "SAMPLE_NUMS = 50  # 100\n",
    "LR = 0.01\n",
    "LAMBDA = torch.tensor(0.98)\n",
    "CLIP_GRAD_NORM = 40  # 40\n",
    "BETA = 0.001\n",
    "GAMMA_INIT = 0.99  # 0.99\n",
    "GAMMA_FIX = 1.0  # 0.995"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Function\n",
    "### Rollout function & testing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roll_out(agent, task, sample_nums, init_state):\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    vts = []  # v-values at timestep t\n",
    "    vt1s = []  # v-values at timestep t+1\n",
    "    dones = []\n",
    "\n",
    "    state = init_state\n",
    "\n",
    "    for i in range(sample_nums):\n",
    "        states.append(state)\n",
    "        act, vt = choose_action(agent, state)\n",
    "        actions.append(act)\n",
    "\n",
    "        next_state, reward, done, _ = task.step(act.numpy())\n",
    "        with torch.no_grad():\n",
    "            _, vt1 = agent(torch.Tensor(next_state))\n",
    "        state = next_state\n",
    "        rewards.append(reward)\n",
    "        dones.append(1 if done is False else 0)\n",
    "        vt = vt.detach().numpy()\n",
    "        vt1 = vt1.detach().numpy()\n",
    "        vts.append(vt)\n",
    "        vt1s.append(vt1)\n",
    "\n",
    "        if done:\n",
    "            state = task.reset()\n",
    "\n",
    "    return states, actions, rewards, vts, vt1s, dones, state\n",
    "\n",
    "\n",
    "def test(gym_name, agent):\n",
    "    result = 0\n",
    "    test_task = gym.make(gym_name)\n",
    "    for test_epi in range(10):\n",
    "        state = test_task.reset()\n",
    "        for test_step in range(500):\n",
    "            act, _ = choose_action(agent, state)\n",
    "            next_state, reward, done, _ = test_task.step(act.numpy())\n",
    "            result += reward\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computational function\n",
    "Functions to compute advantages and target v-values from a trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def choose_action(agent, state):\n",
    "    logits, v = agent(torch.Tensor(state))\n",
    "    act_probs = F.softmax(logits, dim=-1)\n",
    "    m = Categorical(act_probs)\n",
    "    act = m.sample()\n",
    "\n",
    "    return act, v\n",
    "\n",
    "\n",
    "def gae_calculater_grad(rewards, v_t_s, v_t1_s, dones, gamma, lambda_):\n",
    "    \"\"\"\n",
    "    Calculate advantages and target v-values\n",
    "    \"\"\"\n",
    "    batch_size = len(rewards)\n",
    "    R = torch.reshape(torch.tensor(0), (1, 1))  # increment term\n",
    "    advs = torch.zeros(batch_size)\n",
    "    for t in reversed(range(0, batch_size)):\n",
    "        delta = torch.tensor(rewards[t]) - torch.tensor(v_t_s[t]) + \\\n",
    "                (gamma * torch.tensor(v_t1_s[t]) * torch.tensor(dones[t]))\n",
    "        R = delta + (gamma * lambda_ * R * torch.tensor(dones[t]))\n",
    "        advs[t] = R\n",
    "    value_target = advs + torch.tensor(np.squeeze(v_t_s))  # target v is calculated from adv.\n",
    "\n",
    "    return advs, value_target\n",
    "\n",
    "\n",
    "def trajectory_cutter(rewards, vts, vt1s, dones):\n",
    "    \"\"\"\n",
    "    Divide sample into multiple groups for returns computing.\n",
    "    Samples in the same group means that they was sampled in the same epoch.\n",
    "    \"\"\"\n",
    "    # \"not done\" = 1, \"done\" = 0\n",
    "    cutted_rewards = []\n",
    "    cutted_vts = []\n",
    "    cutted_vt1s = []\n",
    "    cutted_dones = []\n",
    "    temp_r = []\n",
    "    temp_vt = []\n",
    "    temp_vt1 = []\n",
    "    temp_d = []\n",
    "    \n",
    "    for (reward, vt, vt1, done) in zip(rewards, vts, vt1s, dones):\n",
    "        temp_r.append(reward)\n",
    "        temp_vt.append(vt)\n",
    "        temp_vt1.append(vt1)\n",
    "        temp_d.append(done)\n",
    "        if done == 0:\n",
    "            cutted_rewards.append(temp_r)\n",
    "            cutted_vts.append(temp_vt)\n",
    "            cutted_vt1s.append(temp_vt1)\n",
    "            cutted_dones.append(temp_d)\n",
    "            temp_r = []\n",
    "            temp_vt = []\n",
    "            temp_vt1 = []\n",
    "            temp_d = []\n",
    "    cutted_rewards.append(temp_r)\n",
    "    cutted_vts.append(temp_vt)\n",
    "    cutted_vt1s.append(temp_vt1)\n",
    "    cutted_dones.append(temp_d)\n",
    "    \n",
    "    return cutted_rewards, cutted_vts, cutted_vt1s, cutted_dones\n",
    "\n",
    "\n",
    "def trajectory_gae(rewards, vts, vt1s, dones, gamma, lambda_):\n",
    "    rewards, vts, vt1s, dones = trajectory_cutter(rewards, vts, vt1s, dones)\n",
    "    \n",
    "    advs = []\n",
    "    v_targets = []\n",
    "    for (r, vt, vt1, d) in zip(rewards, vts, vt1s, dones):\n",
    "        adv, v_target = gae_calculater_grad(r, vt, vt1, d, gamma, lambda_)\n",
    "        advs.append(adv)\n",
    "        v_targets.append(v_target)\n",
    "    advs = torch.cat(advs).float()\n",
    "    v_targets = torch.cat(v_targets).float()\n",
    "    \n",
    "    return advs, v_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test trajectory_gae()\n",
    "rewards = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
    "dones = [1, 1, 1, 1, 1, 0, 1, 1, 1, 1]\n",
    "vts = [np.array(np.random.randn()) for _ in range(10)]\n",
    "vt1s = [np.array(np.random.randn()) for _ in range(10)]\n",
    "GAMMA = torch.tensor(0.99, requires_grad=True)\n",
    "LAMBDA = torch.tensor(0.98)\n",
    "\n",
    "advs, v_targets = trajectory_gae(rewards, vts, vt1s, dones, GAMMA, LAMBDA)\n",
    "# grad = torch.autograd.grad(advs.mean(), GAMMA, allow_unused=True)\n",
    "\n",
    "# for g in grad:\n",
    "#     assert g is not None, \"test failed\"\n",
    "#     print(\"test passed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent training function\n",
    "RL algorithm: A2C + GAE\n",
    "Calculate grad for substitute agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logits(agent, states, actions, action_dim):\n",
    "    states = torch.Tensor(np.array(states))\n",
    "    actions = torch.tensor(actions, dtype=torch.int64).view(-1, 1)\n",
    "    \n",
    "    logits, v = agent(states)\n",
    "    logits = logits.view(-1, action_dim)\n",
    "    v = v.view(-1)\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    log_probs = F.log_softmax(logits, dim=1)\n",
    "    log_probs_act = log_probs.gather(1, actions).view(-1)\n",
    "    \n",
    "    return probs, log_probs, log_probs_act, v\n",
    "\n",
    "\n",
    "def get_meta_logits(agent, states, actions, action_dim):\n",
    "    states = torch.Tensor(np.array(states))\n",
    "    actions = torch.tensor(actions, dtype=torch.int64).view(-1, 1)\n",
    "    \n",
    "    logits, _ = agent(states)\n",
    "    logits = logits.view(-1, action_dim)\n",
    "    log_probs = F.log_softmax(logits, dim=1)\n",
    "    log_probs_act = log_probs.gather(1, actions).view(-1)\n",
    "    \n",
    "    return log_probs_act"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[123456789]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_seed = 123456789\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "gym_name = \"CartPole-v1\"\n",
    "task = gym.make(gym_name)\n",
    "task.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "discrete = isinstance(task.action_space, gym.spaces.Discrete)\n",
    "STATE_DIM = task.observation_space.shape[0]\n",
    "ACTION_DIM = task.action_space.n if discrete else task.action_space.shape[0]\n",
    "\n",
    "agent = ActorCritic(STATE_DIM, ACTION_DIM)\n",
    "optim = torch.optim.RMSprop(agent.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.LinearLR(optim, start_factor=1.0, end_factor=0.0, total_iters=ITERATION_NUMS)\n",
    "assert next(agent.parameters()).is_cuda is False  # use only cpu for the current version\n",
    "\n",
    "gamma = torch.tensor(GAMMA_INIT, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use SGD to update gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 200 test result: 9.8 gamma: tensor(0.7000)\n",
      "iteration: 400 test result: 9.1 gamma: tensor(0.7000)\n",
      "iteration: 600 test result: 264.5 gamma: tensor(0.6997)\n",
      "iteration: 800 test result: 173.2 gamma: tensor(0.7017)\n",
      "iteration: 1000 test result: 122.6 gamma: tensor(0.6965)\n",
      "iteration: 1200 test result: 177.8 gamma: tensor(0.6962)\n",
      "iteration: 1400 test result: 123.5 gamma: tensor(0.6975)\n",
      "iteration: 1600 test result: 131.3 gamma: tensor(0.6956)\n",
      "iteration: 1800 test result: 165.0 gamma: tensor(0.6947)\n",
      "iteration: 2000 test result: 209.1 gamma: tensor(0.6956)\n",
      "iteration: 2200 test result: 114.8 gamma: tensor(0.6965)\n",
      "iteration: 2400 test result: 145.1 gamma: tensor(0.6960)\n",
      "iteration: 2600 test result: 185.5 gamma: tensor(0.6915)\n",
      "iteration: 2800 test result: 220.5 gamma: tensor(0.6922)\n",
      "iteration: 3000 test result: 119.5 gamma: tensor(0.6924)\n",
      "iteration: 3200 test result: 140.8 gamma: tensor(0.6899)\n",
      "iteration: 3400 test result: 230.6 gamma: tensor(0.6881)\n",
      "iteration: 3600 test result: 271.3 gamma: tensor(0.6916)\n",
      "iteration: 3800 test result: 257.6 gamma: tensor(0.6923)\n",
      "iteration: 4000 test result: 161.0 gamma: tensor(0.6918)\n",
      "iteration: 4200 test result: 120.1 gamma: tensor(0.6913)\n",
      "iteration: 4400 test result: 105.6 gamma: tensor(0.6917)\n",
      "iteration: 4600 test result: 142.4 gamma: tensor(0.6896)\n",
      "iteration: 4800 test result: 172.7 gamma: tensor(0.6920)\n",
      "iteration: 5000 test result: 194.4 gamma: tensor(0.6918)\n",
      "iteration: 5200 test result: 411.6 gamma: tensor(0.6923)\n",
      "iteration: 5400 test result: 393.4 gamma: tensor(0.6906)\n",
      "iteration: 5600 test result: 306.1 gamma: tensor(0.6908)\n",
      "iteration: 5800 test result: 500.0 gamma: tensor(0.6913)\n",
      "iteration: 6000 test result: 500.0 gamma: tensor(0.6892)\n",
      "iteration: 6200 test result: 500.0 gamma: tensor(0.6885)\n",
      "iteration: 6400 test result: 11.4 gamma: tensor(0.6900)\n",
      "iteration: 6600 test result: 260.3 gamma: tensor(0.6955)\n",
      "iteration: 6800 test result: 213.8 gamma: tensor(0.6971)\n",
      "iteration: 7000 test result: 170.0 gamma: tensor(0.6975)\n",
      "iteration: 7200 test result: 182.0 gamma: tensor(0.6980)\n",
      "iteration: 7400 test result: 268.3 gamma: tensor(0.6979)\n",
      "iteration: 7600 test result: 120.0 gamma: tensor(0.6978)\n",
      "iteration: 7800 test result: 134.6 gamma: tensor(0.6960)\n",
      "iteration: 8000 test result: 215.0 gamma: tensor(0.6942)\n",
      "iteration: 8200 test result: 400.5 gamma: tensor(0.6944)\n",
      "iteration: 8400 test result: 500.0 gamma: tensor(0.6944)\n",
      "iteration: 8600 test result: 271.1 gamma: tensor(0.6956)\n",
      "iteration: 8800 test result: 500.0 gamma: tensor(0.6964)\n",
      "iteration: 9000 test result: 500.0 gamma: tensor(0.6961)\n",
      "iteration: 9200 test result: 500.0 gamma: tensor(0.6960)\n",
      "iteration: 9400 test result: 401.0 gamma: tensor(0.6955)\n",
      "iteration: 9600 test result: 344.7 gamma: tensor(0.6959)\n",
      "iteration: 9800 test result: 500.0 gamma: tensor(0.6958)\n",
      "iteration: 10000 test result: 500.0 gamma: tensor(0.6956)\n",
      "iteration: 10200 test result: 500.0 gamma: tensor(0.6963)\n",
      "iteration: 10400 test result: 500.0 gamma: tensor(0.6966)\n",
      "iteration: 10600 test result: 500.0 gamma: tensor(0.6959)\n",
      "iteration: 10800 test result: 500.0 gamma: tensor(0.6954)\n",
      "iteration: 11000 test result: 500.0 gamma: tensor(0.6956)\n",
      "iteration: 11200 test result: 500.0 gamma: tensor(0.6955)\n",
      "iteration: 11400 test result: 500.0 gamma: tensor(0.6961)\n",
      "iteration: 11600 test result: 500.0 gamma: tensor(0.6957)\n",
      "iteration: 11800 test result: 472.9 gamma: tensor(0.6959)\n",
      "iteration: 12000 test result: 485.8 gamma: tensor(0.6962)\n",
      "iteration: 12200 test result: 487.1 gamma: tensor(0.6962)\n",
      "iteration: 12400 test result: 456.0 gamma: tensor(0.6965)\n",
      "iteration: 12600 test result: 405.4 gamma: tensor(0.6965)\n",
      "iteration: 12800 test result: 500.0 gamma: tensor(0.6964)\n",
      "iteration: 13000 test result: 425.3 gamma: tensor(0.6968)\n",
      "iteration: 13200 test result: 487.6 gamma: tensor(0.6970)\n",
      "iteration: 13400 test result: 500.0 gamma: tensor(0.6973)\n",
      "iteration: 13600 test result: 500.0 gamma: tensor(0.6978)\n",
      "iteration: 13800 test result: 479.8 gamma: tensor(0.6984)\n",
      "iteration: 14000 test result: 445.8 gamma: tensor(0.6984)\n",
      "iteration: 14200 test result: 500.0 gamma: tensor(0.6985)\n",
      "iteration: 14400 test result: 500.0 gamma: tensor(0.6984)\n",
      "iteration: 14600 test result: 500.0 gamma: tensor(0.6987)\n",
      "iteration: 14800 test result: 500.0 gamma: tensor(0.6978)\n",
      "iteration: 15000 test result: 500.0 gamma: tensor(0.6979)\n",
      "iteration: 15200 test result: 478.7 gamma: tensor(0.6981)\n",
      "iteration: 15400 test result: 341.2 gamma: tensor(0.6986)\n",
      "iteration: 15600 test result: 404.4 gamma: tensor(0.6975)\n",
      "iteration: 15800 test result: 411.8 gamma: tensor(0.6982)\n",
      "iteration: 16000 test result: 496.1 gamma: tensor(0.6980)\n",
      "iteration: 16200 test result: 492.9 gamma: tensor(0.6985)\n",
      "iteration: 16400 test result: 488.6 gamma: tensor(0.6990)\n",
      "iteration: 16600 test result: 490.7 gamma: tensor(0.6988)\n",
      "iteration: 16800 test result: 390.1 gamma: tensor(0.6990)\n",
      "iteration: 17000 test result: 461.3 gamma: tensor(0.6988)\n",
      "iteration: 17200 test result: 401.5 gamma: tensor(0.6987)\n",
      "iteration: 17400 test result: 374.9 gamma: tensor(0.6986)\n",
      "iteration: 17600 test result: 479.0 gamma: tensor(0.6982)\n",
      "iteration: 17800 test result: 424.3 gamma: tensor(0.6983)\n",
      "iteration: 18000 test result: 500.0 gamma: tensor(0.6985)\n",
      "iteration: 18200 test result: 491.1 gamma: tensor(0.6983)\n",
      "iteration: 18400 test result: 500.0 gamma: tensor(0.6983)\n",
      "iteration: 18600 test result: 428.5 gamma: tensor(0.6981)\n",
      "iteration: 18800 test result: 460.8 gamma: tensor(0.6986)\n",
      "iteration: 19000 test result: 434.8 gamma: tensor(0.6986)\n",
      "iteration: 19200 test result: 131.4 gamma: tensor(0.6948)\n",
      "iteration: 19400 test result: 139.8 gamma: tensor(0.6980)\n",
      "iteration: 19600 test result: 222.4 gamma: tensor(0.7013)\n",
      "iteration: 19800 test result: 102.2 gamma: tensor(0.7022)\n",
      "iteration: 20000 test result: 122.3 gamma: tensor(0.7130)\n"
     ]
    }
   ],
   "source": [
    "iterations = []\n",
    "test_results = []\n",
    "gamma_buffer = []\n",
    "\n",
    "init_state = task.reset()\n",
    "for i in range(ITERATION_NUMS):\n",
    "    # Line 3-4　Sample trajectories for both RL and meta-grad learning\n",
    "    states1, actions1, rewards1, vts1, vt1s1, dones1, current_state = roll_out(agent, task, SAMPLE_NUMS, init_state)\n",
    "    init_state = current_state\n",
    "\n",
    "    \"\"\"RL Phase:\"\"\"\n",
    "    probs, log_probs, log_probs_act, v = get_logits(agent, states1, actions1, ACTION_DIM)\n",
    "    advs, v_targets = trajectory_gae(rewards1, vts1, vt1s1, dones1, gamma, LAMBDA)\n",
    "\n",
    "    # Line 5  Loss computation\n",
    "    loss_policy = - (advs.detach() * log_probs_act).mean()\n",
    "    loss_critic = F.mse_loss(v_targets.detach(), v, reduction='mean')\n",
    "    loss_entropy = - (log_probs * probs).mean()\n",
    "\n",
    "    loss = loss_policy + .25 * loss_critic - .001 * loss_entropy\n",
    "    optim.zero_grad()\n",
    "    \n",
    "    # Compute trace z.\n",
    "    f1 = torch.autograd.grad(v_targets.mean(), gamma, retain_graph=True)\n",
    "    f1 = f1[0]\n",
    "    \n",
    "    f2 = torch.autograd.grad(loss_policy + .25 * loss_critic, agent.parameters(), retain_graph=True)\n",
    "    f2 = [item.view(-1) for item in f2]\n",
    "    f2 = torch.cat(f2)\n",
    "    \n",
    "    z = LR * f1 * f2\n",
    "    \n",
    "    loss.backward(retain_graph=True)\n",
    "    torch.nn.utils.clip_grad_norm_(agent.parameters(), CLIP_GRAD_NORM)\n",
    "\n",
    "    # Line 6  Obtain the update agent\n",
    "    optim.step()\n",
    "    scheduler.step()\n",
    "    optim.zero_grad()\n",
    "\n",
    "    \"\"\"Meta-grad Learning Phase:\"\"\"\n",
    "    states2, actions2, rewards2, vts2, vt1s2, dones2, current_state = roll_out(agent, task, SAMPLE_NUMS, init_state)\n",
    "    init_state = current_state\n",
    "    \n",
    "    # Line 7-9  Update meta-parameter\n",
    "    log_probs_act_dash = get_meta_logits(agent, states2, actions2, ACTION_DIM)\n",
    "    advs_dash, _ = trajectory_gae(rewards2, vts2, vt1s2, dones2, torch.tensor(GAMMA_FIX), LAMBDA)\n",
    "    \n",
    "    # Compute meta-gradient\n",
    "    meta_loss = (advs_dash.detach() * log_probs_act_dash).mean()\n",
    "    J_dash = torch.autograd.grad(meta_loss, agent.parameters(), allow_unused=True)\n",
    "    J_dash = [torch.zeros_like(params.data) if item is None else item\n",
    "              for (item, params) in zip(J_dash, agent.parameters())]\n",
    "    J_dash = [item.view(-1) for item in J_dash]\n",
    "    J_dash = torch.cat(J_dash)\n",
    "    \n",
    "    delta_gamma = - BETA * (J_dash @ z)\n",
    "    gamma.data += delta_gamma\n",
    "    # gamma.data += torch.sign(delta_gamma) * torch.min(torch.abs(delta_gamma), torch.tensor(0.1))\n",
    "    # Limit gamma into (0, 1), bug might happen after this mechanism take effect\n",
    "    if gamma.data > torch.tensor(0.9999):\n",
    "        gamma.data = torch.tensor(0.9999)\n",
    "    elif gamma.data < torch.tensor(0.0001):\n",
    "        gamma.data = torch.tensor(0.0001)\n",
    "\n",
    "    # testing\n",
    "    if (i + 1) % (ITERATION_NUMS // 100) == 0:\n",
    "        result = test(gym_name, agent)\n",
    "        print(\"iteration:\", i + 1, \"test result:\", result / 10.0, \"gamma:\", gamma.data)\n",
    "        iterations.append(i + 1)\n",
    "        test_results.append(result / 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
